{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Handle the necessary imports\"\"\"\n",
    "import cv2\n",
    "import math\n",
    "import numpy as np\n",
    "from random import randrange\n",
    "import os\n",
    "import glob\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_size=64 #60*60*3\n",
    "num_training_images=10000\n",
    "num_testing_images=1000\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##This Notebook extracts faces from multiple datasets, resizes them according to patch size and saves them in the corresponding folders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FDDB Done\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"For FDDB Preprocessing\"\"\"\n",
    "\n",
    "def ellipse_to_bbox(obj):\n",
    "    # From proj1 reference code\n",
    "    maj_rad = obj[0]\n",
    "    min_rad = obj[1]\n",
    "    angle = obj[2]\n",
    "    xcenter = obj[3]\n",
    "    ycenter = obj[4]\n",
    "\n",
    "    cosin = math.cos(math.radians(-angle))\n",
    "    sin = math.sin(math.radians(-angle))\n",
    "\n",
    "    x1 = cosin * (-min_rad) - sin * (-maj_rad) + xcenter\n",
    "    y1 = sin * (-min_rad) + cosin * (-maj_rad) + ycenter\n",
    "    x2 = cosin * (min_rad) - sin * (-maj_rad) + xcenter\n",
    "    y2 = sin * (min_rad) + cosin * (-maj_rad) + ycenter\n",
    "    x3 = cosin * (min_rad) - sin * (maj_rad) + xcenter\n",
    "    y3 = sin * (min_rad) + cosin * (maj_rad) + ycenter\n",
    "    x4 = cosin * (-min_rad) - sin * (maj_rad) + xcenter\n",
    "    y4 = sin * (-min_rad) + cosin * (maj_rad) + ycenter\n",
    "    wid = [x1, x2, x3, x4]\n",
    "    hei = [y1, y2, y3, y4]\n",
    "    xmin_ = int(min(wid))\n",
    "    xmax_ = int(max(wid))\n",
    "    ymin_ = int(min(hei))\n",
    "    ymax_ = int(max(hei))\n",
    "\n",
    "    return xmin_, ymin_, xmax_, ymax_\n",
    "\n",
    "\n",
    "num_folds=10\n",
    "image_index=1\n",
    "\n",
    "\n",
    "# to create directory if it doesn't exist\n",
    "def check_and_create_directory(directory):\n",
    "    if not os.path.exists(directory):\n",
    "        os.mkdir(directory)\n",
    "        print(\"Created Directory\",directory)\n",
    "    \n",
    "\n",
    "# to generate random patch that does not overlap with face bbox\n",
    "def generate_random_patch(region,total_size,patch_size):\n",
    "    # region - xmin, ymin,xmax,ymax\n",
    "    # total_size - xlim, ylim, 3\n",
    "    xlim=total_size[0]\n",
    "    ylim=total_size[1]\n",
    "    x_range_face=range(region[0],region[2])\n",
    "    y_range_face=range(region[1],region[3])\n",
    "    x=[i for i in range(xlim) if i not in x_range_face]\n",
    "    y=[i for i in range(ylim) if i not in y_range_face]\n",
    "    # we have the ranges that are not part of the face\n",
    "    if(len(x)>patch_size and len(y)>patch_size):\n",
    "        return x[0],y[0],x[patch_size],y[patch_size] \n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "check_and_create_directory(\"Dataset//extracted_faces//\")\n",
    "check_and_create_directory(\"Dataset//extracted_nonfaces//\")\n",
    "\n",
    "\n",
    "for fold in range(1,num_folds+1):\n",
    "    txt_file = (\"Data//FDDB-folds//FDDB-fold-\"+str(fold).zfill(2)+\"-ellipseList.txt\")\n",
    "    with open(txt_file) as f: lines = [line.rstrip('\\n') for line in f]\n",
    "    i=0\n",
    "    while(i<len(lines)):\n",
    "            p=lines[i]\n",
    "            path=p\n",
    "            #print(p)\n",
    "            i=i+1\n",
    "            p=lines[i]\n",
    "            k=int(p)\n",
    "            #print(k)\n",
    "            i=i+1\n",
    "            #print(\"Ellipse\")\n",
    "            im1=cv2.imread(\"Data//originalPics//\"+path+\".jpg\") #Read in the image\n",
    "            im2=np.zeros_like(im1)\n",
    "            bboxes=[]\n",
    "            while(k>0):\n",
    "                p=lines[i]\n",
    "                #print(p)\n",
    "                i=i+1\n",
    "                k=k-1\n",
    "                #Read the ellipse coordinates\n",
    "                \"\"\"<major_axis_radius minor_axis_radius angle center_x center_y 1>.\"\"\"\n",
    "                fields=p.split(' ')\n",
    "                fields = [float(fields[i]) for i in range(5)]\n",
    "                bbox=ellipse_to_bbox(fields)\n",
    "                bboxes.append(bbox)\n",
    "                #print(bbox)\n",
    "                \n",
    "            for bbox in bboxes:    \n",
    "                face=im1[bbox[1]:bbox[3],bbox[0]:bbox[2]] #cropped face\n",
    "                bbox_rand=generate_random_patch(bbox,im1.shape,patch_size)\n",
    "                # bbox[3]  # randrange(0,im1.shape[0])\n",
    "                # bbox[2]  # randrange(0,im1.shape[1])\n",
    "                if(bbox_rand!=None):\n",
    "                    non_face=im1[bbox_rand[1]:bbox_rand[3],bbox_rand[0]:bbox_rand[2]] #cropped nonface\n",
    "                if(non_face.size>0):\n",
    "                    resized_nonface=cv2.resize(non_face,(patch_size,patch_size),cv2.INTER_LANCZOS4) #resize it\n",
    "                    cv2.imwrite('Dataset//extracted_nonfaces//nonface_'+str(image_index)+'.jpg',resized_nonface)  # save the image to disk \n",
    "                if(face.size>0):\n",
    "                    resized_image=cv2.resize(face,(patch_size,patch_size),cv2.INTER_LANCZOS4) #resize it\n",
    "                    cv2.imwrite('Dataset//extracted_faces//face_'+str(image_index)+'.jpg',resized_image)  # save the image to disk \n",
    "                    \n",
    "                    image_index=image_index+1\n",
    "                    #print(image_index)\n",
    "                #plt.imshow(res)\n",
    "                #plt.show()\n",
    "                \"\"\"\n",
    "                #ellipse mask\n",
    "    \n",
    "                fields=p.split(' ')\n",
    "                major_axis_radius=round(float(fields[0]))\n",
    "                minor_axis_radius=round(float(fields[1]))\n",
    "                angle=(float(fields[2]))*180.0/np.pi\n",
    "                center_x=round(float(fields[3]))\n",
    "                center_y=round(float(fields[4]))\n",
    "                cv2.ellipse(im2,(center_x,center_y),(major_axis_radius,minor_axis_radius),angle,0,360,(0,0,255),-1)\n",
    "    \n",
    "            #res= cv2.bitwise_and(im1,im2) # has the face and black background.\n",
    "            \"\"\"\n",
    "        \n",
    "print(\"FDDB Done\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Now for AFLW and LFW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4825\n"
     ]
    }
   ],
   "source": [
    "print(image_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of Images 4825\n"
     ]
    }
   ],
   "source": [
    "\"\"\" AFLW and LFW Preprocessing\"\"\"\n",
    "#aflw_faces = []\n",
    "#lfw_faces = []\n",
    "\n",
    "t=4825\n",
    "# faces AFLW\n",
    "for img in glob.glob(\"Data//AFLW//positive_faces//*.png\"):\n",
    "    n= cv2.imread(img)# Read the face\n",
    "    resized_image=cv2.resize(n,(patch_size,patch_size),cv2.INTER_LANCZOS4) #resize it\n",
    "    if(resized_image.size==(patch_size,patch_size)):\n",
    "        cv2.imwrite('Dataset//extracted_faces//face_'+str(t)+'.jpg',resized_image)  # save the image to disk \n",
    "        t=t+1          \n",
    "    #aflw_faces.append(n)\n",
    "    \n",
    "#print(\" Faces in AFLW\",len(aflw_faces))\n",
    "\n",
    "t=4825\n",
    "# Non-Faces AFLW\n",
    "for img in glob.glob(\"Data//AFLW//negative_faces//*.png\"):\n",
    "    n=cv2.imread(img)# Read the nonface\n",
    "    resized_image=cv2.resize(n,(patch_size,patch_size),cv2.INTER_LANCZOS4) #resize it\n",
    "    if(resized_image.size==(patch_size,patch_size)):\n",
    "        cv2.imwrite('Dataset//extracted_nonfaces//nonface_'+str(t)+'.jpg',resized_image)  # save the image to disk \n",
    "        t=t+1        \n",
    "        \n",
    "# Positive faces LFW\n",
    "\"\"\"\n",
    "for img in glob.glob(\"Data//LFW//*//*.jpg\"):\n",
    "    n= cv2.imread(img)\n",
    "    lfw_faces.append(n)\n",
    "    \n",
    "print(\"Faces in LFW\",len(lfw_faces))\n",
    "\"\"\"\n",
    "print(\"Total Number of Images\",t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have tested load and display of the images,we can proceed with resizing and the training and test data split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "index=0\n",
    "while index<num_training_images:\n",
    "    loaded_image=cv2.imread('Data//AFLW//positive_faces//positive_face_'+str(index)+'.png')  # load the image from disk \n",
    "    resized_image=cv2.resize(loaded_image,(patch_size,patch_size),cv2.INTER_LANCZOS4) #resize it\n",
    "    cv2.imwrite('Data//AFLW//extracted_faces//train//train_face'+str(index)+'.png',resized_image)  # save the image to disk \n",
    "    #repeat for non-faces \n",
    "    #loaded_image=cv2.imread('Data//WIDER_train//images//0--Parade'+str(index)+'.png')  # load the image from disk \n",
    "    #resized_image=cv2.resize(loaded_image,(patch_size,patch_size),cv2.INTER_LANCZOS4) #resize it\n",
    "    #cv2.imwrite('Data//AFLW//extracted_faces//train_face'+str(index)+'.png',resized_image)  # save the image to disk\n",
    "    index+=1 #increment\n",
    "    #cv2.imshow(\"Loaded Image\",resized_image) # show image\n",
    "    #cv2.waitKey(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly process for testing images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "index=1000\n",
    "while index<num_testing_images+num_training_images:\n",
    "    loaded_image=cv2.imread('Data//AFLW//positive_faces//positive_face_'+str(index)+'.png')  # load the image from disk \n",
    "    resized_image=cv2.resize(loaded_image,(patch_size,patch_size),cv2.INTER_LANCZOS4) #resize it\n",
    "    cv2.imwrite('Data//AFLW//extracted_faces//test//test_face'+str(index-num_training_images)+'.png',resized_image)  # save the image to disk \n",
    "    #repeat for non-faces \n",
    "    #loaded_image=cv2.imread('Data//WIDER_train//images//0--Parade'+str(index)+'.png')  # load the image from disk \n",
    "    #resized_image=cv2.resize(loaded_image,(patch_size,patch_size),cv2.INTER_LANCZOS4) #resize it\n",
    "    #cv2.imwrite('Data//AFLW//extracted_faces//train_face'+str(index)+'.png',resized_image)  # save the image to disk\n",
    "    index+=1 #increment\n",
    "    #cv2.imshow(\"Loaded Image\",resized_image) # show image\n",
    "    #cv2.waitKey(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can proceed with training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PREPARE TRAINING DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 10800)\n",
      "(1000, 10800)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
